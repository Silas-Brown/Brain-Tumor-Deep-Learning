{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGiFmoEQqn7B"
      },
      "source": [
        "# **Background and objective**\n",
        "https://www.kaggle.com/datasets/denizkavi1/brain-tumor\n",
        "\n",
        "Summary by the author of a paper on this dataset\n",
        "\n",
        "*This brain tumor dataset containing 3064 T1-weighted contrast-inhanced images from 233 patients with three kinds of brain tumor: meningioma (708 slices), glioma (1426 slices), and pituitary tumor (930 slices).\n",
        "This data was used in the following paper:*\n",
        "\n",
        "We create a DNN that will distinguish and predict these 3 classes (kinds of tumors). A principled loss function based on the medical costs of each error will be created and used. A principled performance metric on the same basis will be created and maximized via a hyperparameter grid search."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yY5j7Fy1gp3"
      },
      "source": [
        "# **Problem in general form**\n",
        "\n",
        "A DNN with $L$ layers between $y$ and the image data matrix $X$ can be represented in the generic form\n",
        "\n",
        "$$y=(f_{L} \\circ f_{L-1} \\circ....\\circ f_{1})(X) \\in \\{Meningioma,Glioma, Pituitary\\}$$\n",
        "\n",
        "$$f_{k}: \\mathbb{R}^{d_{k}} \\to \\mathbb{R}^{d_{k+1}}$$\n",
        "\n",
        "Where $d_{k}$ is the dimension (width) of the $k^{th}$ layer. Let $\\textbf{d} \\in \\mathbb{N}^{L}$ be the vector of layer widths (dimensions). $f_{k}$ is the activation function between layer $k$ and layer $k+1$.\n",
        "\n",
        "We also have a hyperparameter for training the DNN $H$ defined as\n",
        "\n",
        "$$H=(r,B,L) \\in \\mathbb{N}^{3}$$\n",
        "\n",
        "Where\n",
        "\n",
        "*   $r$ is the dimension of the square resolution (resolution = $r \\times r$)\n",
        "*   $B$ is the batch size of training\n",
        "*   $L$ is the number of layers between $X$ and $y$. Our $f_{L}$ would be the activation\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klvzvYPTRASl"
      },
      "source": [
        "# **Specifying the functions and hyperparameters**\n",
        "\n",
        "\n",
        "\n",
        "1.   **Choosing what $f_{k}$ and $\\textbf{d}$ to use**\n",
        "      \n",
        "      For $k \\neq L$, $f_{k}$ are functions that don't activate the final output. These are activations between hidden layers. For all $k \\neq L$, we choose $f_{k}=ReLu$.\n",
        "\n",
        "      To design our neural achitecture $\\textbf{d}$ we must consider that the visual complexity of the tumor itself isn't high, but at the same time there is considerable positional variation of the tumor itself. We should make $\\textbf{d}$ pretty wide for the first few layers and then abruptly narrow it afterward. The idea is to scan widely for a simple object, but then aggressively zero in afterward once we get a signal.\n",
        "      \n",
        "      Let's try\n",
        "      *   $\\textbf{d}=(256,64)$ if $L=2$\n",
        "      *   $\\textbf{d}=(256,256,16)$ if $L=3$\n",
        "      *   $\\textbf{d}=(256,256,32,16)$ if $L=4$\n",
        "\n",
        "  The three different choices of $L$ here will be justified in the (2) point below.\n",
        "\n 2.   **Choosing a search space $\\mathcal{H}$ for $H$ and why**\n",
        "\n",
        "      Let\n",
        "$$r \\in \\{128,256\\},$$\n",
        "$$B \\in \\{32,64\\},$$\n",
        "$$L \\in \\{2,3,4\\}$$\n",
        "\n",
        "We do not need a very large resolution. The tumor is visually very distinct and it doesn't really blend in with the background.\n",
        "\n",
        "\n",
        "We use modest batch sizes of 32 or 64 due to the low resolution of the images. That is, we can afford higher resolution learning with lower resolution data.\n",
        "\n",
        "Hidden layers are between 2 and 4 as the images are low resolution and we are detecting tumors. Both complex learning and computational cost are reasonably respected.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrO3OCUv8cuQ"
      },
      "source": [
        "# **Choosing an informed loss function $\\mathcal{L}(\\hat{Y}, Y)$**\n",
        "\n",
        "Let $Y \\in \\mathbb{R}^{3 \\times n}$ be a $3 \\times n$ matrix and each row $Y_{i}$ be a vector of entries 0 or 1. If the $k^{th}$ entry is 1, this means it is class $k$. Let $\\hat{Y_{i}}$ be a vector of probabilities for each class.\n",
        "\n",
        "Let Classes 0,1,2 be Meningioma, Glioma, and Pituitary respectively. We will create a weighted cross-entropy loss where each weight is the mortality risk divided by 10. For example, the death rate of meningioma, glioma and pituitary are 5%, 80%, and 1% respectively. So our weight vector becomes $\\textbf{w}=[0.5,8,0.1]$. Our cost function is then\n",
        "\n",
        "$$\\mathcal{L}(\\hat{Y},Y)=\\frac{-1}{n} \\sum_{i=1}^{n} \\sum_{j=0}^{2}w_{j}Y_{ij}\\log(\\hat{Y}_{ij})$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LnW5Dkn2kEWq"
      },
      "source": [
        "# **Choosing an informed performance metric with respect to $H$, $\\mathcal{M}(H)$**\n",
        "\n",
        "This will be designed using the same principles used to design our loss function $\\mathcal{L}(\\hat{Y},Y)$. It will be a normalized weighted combination of the F1 scores of each class, where the weights are the same as the loss function.\n",
        "\n",
        "$$\\mathcal{M}(H)=\\frac{8 \\cdot F1_{glioma}+0.1 \\cdot F1_{pituitary} + 0.5 \\cdot F1_{meningioma}}{8.6}$$\n",
        "\n",
        "We divide it by 8.6 because the maximum value of the numerator is 8.6. This way, $\\mathcal{M}(H) \\in [0,1]$. For each F1 score we set a $\\beta$. Considering the aggression and lethality of glioma, we will aggressively emphasize recall and make $\\beta_{glioma}=3$, making it $9 \\times$ more important. We will have $\\beta_{meningioma}=\\beta_{pituitary}=1$.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyFlyQzuJT34"
      },
      "source": [
        "# **Technical implementations**\n",
        "\n",
        "Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "iKQ8hvGaOsAi",
        "outputId": "386a7e10-4308-4d2d-dd96-5d2d05582774"
      },
      "outputs": [],
      "source": [
        "!unzip /content/TumorImages.zip -d /content/dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "W53a8Su2O_tI"
      },
      "outputs": [],
      "source": [
        "\n",
        "dataset_path = \"/content/dataset/TumorImages\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y56omhr_AOhL"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "def prepare_data(dataset_path, B, r):\n",
        "\n",
        "  img_size = (r, r)\n",
        "  from tensorflow.keras.utils import image_dataset_from_directory\n",
        "  import io\n",
        "  import contextlib\n",
        "\n",
        "  with contextlib.redirect_stdout(io.StringIO()):\n",
        "    train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "        dataset_path,\n",
        "        labels='inferred',\n",
        "        label_mode='categorical',\n",
        "        batch_size=B,\n",
        "        image_size=img_size,\n",
        "        shuffle=True,\n",
        "        validation_split=0.2,\n",
        "        subset='training',\n",
        "        seed=123\n",
        "      )\n",
        "\n",
        "    val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "        dataset_path,\n",
        "        labels='inferred',\n",
        "        label_mode='categorical',\n",
        "        batch_size=B,\n",
        "        image_size=img_size,\n",
        "        shuffle=True,\n",
        "        validation_split=0.2,\n",
        "        subset='validation',\n",
        "        seed=123\n",
        "      )\n",
        "\n",
        "# Optional: Prefetch for performance\n",
        "  AUTOTUNE = tf.data.AUTOTUNE\n",
        "  train_ds = train_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "  val_ds = val_ds.prefetch(buffer_size=AUTOTUNE)\n",
        "\n",
        "  return train_ds, val_ds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCSVKol-roa1"
      },
      "outputs": [],
      "source": [
        "def get_model(L):\n",
        "    if L == 2:\n",
        "        hidden_layers = [256, 64]\n",
        "    elif L == 3:\n",
        "        hidden_layers = [256, 256, 16]\n",
        "    elif L == 4:\n",
        "        hidden_layers = [256, 256, 32, 16]\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported L value. Choose 2, 3, or 4.\")\n",
        "\n",
        "    layers = [tf.keras.layers.Flatten()]\n",
        "    for units in hidden_layers:\n",
        "        layers.append(tf.keras.layers.Dense(units, activation='relu'))\n",
        "    layers.append(tf.keras.layers.Dense(3, activation='softmax'))\n",
        "\n",
        "    model = tf.keras.Sequential(layers)\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vj6hJ_ZCdCgx"
      },
      "outputs": [],
      "source": [
        "\n",
        "def custom_weighted_loss(y_true, y_pred):\n",
        "    class_weights = tf.constant([0.5, 8.0, 0.1])\n",
        "    y_true = tf.cast(y_true, tf.float32)\n",
        "    y_pred = tf.clip_by_value(y_pred, 1e-7, 1 - 1e-7)\n",
        "\n",
        "    ce = -tf.reduce_sum(y_true * tf.math.log(y_pred), axis=1)\n",
        "    weights = tf.reduce_sum(y_true * class_weights, axis=1)\n",
        "    weighted_ce = weights * ce\n",
        "\n",
        "    return tf.reduce_mean(weighted_ce)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23x85oA7-vaV"
      },
      "outputs": [],
      "source": [
        "def custom_metric(beta_glioma):\n",
        "    def metric_fn(y_true, y_pred):\n",
        "        y_true = tf.cast(y_true, tf.float32)\n",
        "        y_pred_labels = tf.argmax(y_pred, axis=1)\n",
        "        y_true_labels = tf.argmax(y_true, axis=1)\n",
        "\n",
        "        def f1_beta(class_id, beta):\n",
        "            true_pos = tf.reduce_sum(tf.cast((y_pred_labels == class_id) & (y_true_labels == class_id), tf.float32))\n",
        "            pred_pos = tf.reduce_sum(tf.cast((y_pred_labels == class_id), tf.float32))\n",
        "            actual_pos = tf.reduce_sum(tf.cast((y_true_labels == class_id), tf.float32))\n",
        "\n",
        "            precision = true_pos / (pred_pos + 1e-7)\n",
        "            recall = true_pos / (actual_pos + 1e-7)\n",
        "\n",
        "            beta_sq = beta ** 2\n",
        "            return (1 + beta_sq) * precision * recall / (beta_sq * precision + recall + 1e-7)\n",
        "\n",
        "        f1_meningioma = f1_beta(0, 1.0)\n",
        "        f1_glioma     = f1_beta(1, beta_glioma)\n",
        "        f1_pituitary  = f1_beta(2, 1.0)\n",
        "\n",
        "        return (8.0 * f1_glioma + 0.1 * f1_pituitary + 0.5 * f1_meningioma)/8.6\n",
        "\n",
        "    return metric_fn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orzDcr98rrzR"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "def train_model(model, train_ds, val_ds, E):\n",
        "\n",
        "  model.compile(\n",
        "      optimizer='adam',\n",
        "      loss=custom_weighted_loss,\n",
        "      metrics=[custom_metric(beta_glioma=3.0)]\n",
        "  )\n",
        "\n",
        "  early_stop = EarlyStopping(\n",
        "      monitor='metric_fn',  # Replace with your actual metric name\n",
        "      min_delta=0.005,              # Minimum improvement threshold\n",
        "      patience=5,                   # Number of epochs to wait\n",
        "      mode='max',                   # Because higher is better for metrics\n",
        "      restore_best_weights=True\n",
        "  )\n",
        "\n",
        "# Then plug into model.fit\n",
        "  history=model.fit(\n",
        "      train_ds,\n",
        "      validation_data=val_ds,\n",
        "      epochs=E,\n",
        "      callbacks=[early_stop],\n",
        "      verbose=0\n",
        "  )\n",
        "  M = history.history['metric_fn'][-1]\n",
        "  return M\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KFYzZPta86R",
        "outputId": "c9490b02-dfd1-4747-9195-7176a1604339"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running config: r=128, B=32, L=2, epochs=50\n",
            "Metric (M): 0.8292699456214905\n",
            "Running config: r=128, B=32, L=3, epochs=50\n",
            "Metric (M): 0.005325795151293278\n",
            "Running config: r=128, B=32, L=4, epochs=50\n",
            "Metric (M): 0.8292699456214905\n",
            "Running config: r=256, B=32, L=2, epochs=50\n",
            "Metric (M): 0.8292699456214905\n",
            "Running config: r=256, B=32, L=3, epochs=50\n",
            "Metric (M): 0.005325795151293278\n",
            "Running config: r=256, B=32, L=4, epochs=50\n",
            "Metric (M): 0.02126740850508213\n",
            "Running config: r=128, B=64, L=2, epochs=50\n",
            "Metric (M): 0.834864616394043\n",
            "Running config: r=128, B=64, L=3, epochs=50\n",
            "Metric (M): 0.8326484560966492\n",
            "Running config: r=128, B=64, L=4, epochs=50\n",
            "Metric (M): 0.021434083580970764\n",
            "Running config: r=256, B=64, L=2, epochs=50\n",
            "Metric (M): 0.005366576835513115\n",
            "Running config: r=256, B=64, L=3, epochs=50\n",
            "Metric (M): 0.021434083580970764\n",
            "Running config: r=256, B=64, L=4, epochs=50\n",
            "Metric (M): 0.005366576835513115\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "import itertools\n",
        "\n",
        "\n",
        "batch_sizes = [32,64]\n",
        "image_sizes = [128,256]\n",
        "layers = [2, 3, 4]\n",
        "\n",
        "\n",
        "combinations = list(itertools.product(batch_sizes, image_sizes, layers))\n",
        "\n",
        "\n",
        "for B, r, L, in combinations:\n",
        "    print(f\"Running config: r={r}, B={B}, L={L}, epochs={50}\")\n",
        "    train_ds, val_ds = prepare_data(dataset_path, B, r)\n",
        "    model = get_model(L)\n",
        "    metric=train_model(model, train_ds, val_ds, 50)\n",
        "    print(f\"Metric (M): {metric}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_BxHF5FOzFH"
      },
      "source": [
        "The configuration $H$ that maximizes \n",
        "\n",
        "$\\mathcal{M}(H)$ is $H=(128,64,2)$ with $\\mathcal{M}(H) \\approx 0.835$"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
